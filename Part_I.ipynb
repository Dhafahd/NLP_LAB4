{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRJcdvc7erN1"
      },
      "source": [
        "#### MST AIDS 2023-2024 (Département Génie Informatique)\n",
        "#### Subject : The main purpose behind this lab is to get familiar with NLP language models using Pytorch library.\n",
        "#### Realize by : Chibani Fahd\n",
        "#### web sourcest : Aljazeera\n",
        "#### Topic : \" الحرب الفلسطينية \"\n",
        "#### Models : RNN, Bidirectional RNN, GRU and LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udMmEfLxf_Bs"
      },
      "source": [
        "## Part I : Classification Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "bhTMeLYXrCe5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import json\n",
        "import pyarabic.araby as araby\n",
        "from tashaphyne.stemming import ArabicLightStemmer\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import qalsadi.lemmatizer as lem\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, GRU, Dense\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sitjSyYMgYjo"
      },
      "source": [
        "### 1 - Scrapping using Selenium and BeautifulSoup :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lihHTptgi6eM"
      },
      "source": [
        "a )  Scarpe all links in relation with \" الحرب الفلسطينية\" from aljazeera web site"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoEgC_pOH0Z8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Configure options for Chrome\n",
        "options = Options()\n",
        "options.add_argument('--headless')  # Run in headless mode for non-GUI environments\n",
        "options.add_argument('--disable-gpu')\n",
        "options.add_argument('--no-sandbox')\n",
        "\n",
        "# Set up the Chrome WebDriver\n",
        "driver = webdriver.Chrome()\n",
        "\n",
        "\n",
        "# Open the website\n",
        "driver.get('https://www.aljazeera.net/where/arab/palestine/')\n",
        "\n",
        "links = set()\n",
        "for i in range(30):\n",
        "    # Wait until the links are present\n",
        "    WebDriverWait(driver, 10).until(\n",
        "        EC.presence_of_all_elements_located((By.CLASS_NAME, 'u-clickable-card__link'))\n",
        "    )\n",
        "\n",
        "    # Find all link elements\n",
        "    link_elements = driver.find_elements(By.CLASS_NAME, 'u-clickable-card__link')\n",
        "    for element in link_elements:\n",
        "        links.add(element.get_attribute('href'))\n",
        "\n",
        "    # Try to find and click the \"Show More\" button\n",
        "    show_more_button = WebDriverWait(driver, 10).until(\n",
        "        EC.element_to_be_clickable((By.XPATH, '/html/body/div[1]/div/div[3]/div/div[3]/div/div[1]/section/button'))\n",
        "    )\n",
        "    driver.execute_script('arguments[0].scrollIntoView();', show_more_button)\n",
        "    show_more_button.click()\n",
        "    time.sleep(3)  # Wait for new content to load\n",
        "\n",
        "# Output all links\n",
        "for link in links:\n",
        "    print(link)\n",
        "\n",
        "driver.quit()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbnYHm6fjcxN"
      },
      "source": [
        "b ) Using BeautifulSoup to scrape informations already scraped :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5sM6PhLcv3n"
      },
      "outputs": [],
      "source": [
        "para=[]\n",
        "for url in links:\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "    para_div = soup.find('div', {'class': 'wysiwyg wysiwyg--all-content css-1vkfgk0'})\n",
        "    if para_div:\n",
        "        print(\"URL:\", url)\n",
        "\n",
        "\n",
        "        # Initialize a variable to store all paragraphs\n",
        "        all_paragraphs = \"\"\n",
        "\n",
        "        # Extract paragraphs\n",
        "        for item in para_div.contents:\n",
        "            if item.name == 'p':\n",
        "                all_paragraphs += item.text.strip() + \"  \"\n",
        "\n",
        "        print(\"Paragraphs:\", all_paragraphs)\n",
        "        para.append(all_paragraphs)\n",
        "        print(\"\\n\")  # Add a new line for separation between articles\n",
        "        # Insert data into MongoDB\n",
        "        article_data = {\n",
        "            \"url\": url,\n",
        "            \"paragraphs\": all_paragraphs\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hplKzDXNj8R_"
      },
      "source": [
        "c ) Save informations in a json file named para.json :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFU8v5Xqcv0T"
      },
      "outputs": [],
      "source": [
        "# Step 3: Open (or create) the 'para.json' file in write mode\n",
        "with open('para.json', 'w') as json_file:\n",
        "    # Step 4: Use json.dump() to write the list to the file\n",
        "    json.dump(para, json_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ycCdyhQkGXt"
      },
      "source": [
        "d ) Collecting other information so we can get varies ratings :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jfqnsCrcvxF"
      },
      "outputs": [],
      "source": [
        "# Configure options for Chrome\n",
        "options = Options()\n",
        "options.add_argument('--headless')  # Run in headless mode for non-GUI environments\n",
        "options.add_argument('--disable-gpu')\n",
        "options.add_argument('--no-sandbox')\n",
        "\n",
        "# Set up the Chrome WebDriver\n",
        "driver = webdriver.Chrome()\n",
        "\n",
        "\n",
        "# Open the website\n",
        "driver.get('https://www.aljazeera.net/')\n",
        "\n",
        "links = set()\n",
        "for i in range(1):\n",
        "    # Wait until the links are present\n",
        "    WebDriverWait(driver, 10).until(\n",
        "        EC.presence_of_all_elements_located((By.CLASS_NAME, 'u-clickable-card__link'))\n",
        "    )\n",
        "\n",
        "    # Find all link elements\n",
        "    link_elements = driver.find_elements(By.CLASS_NAME, 'u-clickable-card__link')\n",
        "    for element in link_elements:\n",
        "        links.add(element.get_attribute('href'))\n",
        "\n",
        "# Output all links\n",
        "for link in links:\n",
        "    print(link)\n",
        "\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xc9XKDgxcvs7"
      },
      "outputs": [],
      "source": [
        "para=[]\n",
        "for url in links:\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "    para_div = soup.find('div', {'class': 'wysiwyg wysiwyg--all-content css-1vkfgk0'})\n",
        "    if para_div:\n",
        "        print(\"URL:\", url)\n",
        "\n",
        "\n",
        "        # Initialize a variable to store all paragraphs\n",
        "        all_paragraphs = \"\"\n",
        "\n",
        "        # Extract paragraphs\n",
        "        for item in para_div.contents:\n",
        "            if item.name == 'p':\n",
        "                all_paragraphs += item.text.strip() + \"  \"\n",
        "\n",
        "        print(\"Paragraphs:\", all_paragraphs)\n",
        "        para.append(all_paragraphs)\n",
        "        print(\"\\n\")  # Add a new line for separation between articles\n",
        "        # Insert data into MongoDB\n",
        "        article_data = {\n",
        "            \"url\": url,\n",
        "            \"paragraphs\": all_paragraphs\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh0-qVyBcvpj"
      },
      "outputs": [],
      "source": [
        "# Create a list to hold the dictionaries\n",
        "data_list = []\n",
        "\n",
        "# Iterate over each item in the 'para' list\n",
        "for content in para:\n",
        "    # Create a dictionary with 'content' and 'title'\n",
        "    data = {\n",
        "        \"title\": \"None\",\n",
        "        \"content\": content,\n",
        "    }\n",
        "\n",
        "    # Add the dictionary to the list\n",
        "    data_list.append(data)\n",
        "\n",
        "# Write the list of dictionaries to a single JSON file\n",
        "with open('para2.json', 'w', encoding='utf-8') as json_file:\n",
        "    json.dump(data_list, json_file, ensure_ascii=False, indent=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toNhPTnUk99c"
      },
      "source": [
        "### 2 ) Preprocessing of the collected Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRedJb6Eqqad",
        "outputId": "54d3ba0e-f74e-4225-8435-f4dfa3f767b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                               content    rating\n",
            "0    في هذا المحتوى، سنقدم نظرة عامة على الثقافة ال...  2.398908\n",
            "1    في هذا المحتوى، سنقدم نظرة عامة على الثقافة ال...  1.097328\n",
            "2    يستكشف هذا المحتوى دور الذكاء الاصطناعي في مست...  3.398908\n",
            "3    يتناول هذا المحتوى تحليلًا لتطور صناعة الألعاب...  0.863688\n",
            "4    يقدم هذا المحتوى استعراضًا لتأثير التغيرات الد...  2.398908\n",
            "..                                                 ...       ...\n",
            "223  حاول الغرب، على مدى السنوات القليلة الماضية، ك...  1.148186\n",
            "224  يحذر خبراء في جنوب أفريقيا من عودة الرئيس السا...  1.615157\n",
            "225  في سياق إقليمي معقد تمر به منطقة أفريقيا جنوب ...  3.324926\n",
            "226  قرر يوفنتوس الإيطالي الطعن في حكم كريستيانو رو...  0.855067\n",
            "227  في هذا التحليل، سنستكشف أحدث التطورات في مجال ...  2.425226\n",
            "\n",
            "[228 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "# Read the JSON file with lines=True\n",
        "df = pd.read_json(\"filtered_&_scored_data.json\", lines=True)\n",
        "\n",
        "# Drop the column named 'B'\n",
        "if 'B' in df.columns:\n",
        "    df = df.drop(columns=['B'])\n",
        "\n",
        "# Save the updated DataFrame back to a JSON file\n",
        "df.to_json('updated_para.json', orient='records', lines=True, force_ascii=False, indent=4)\n",
        "\n",
        "# Print the updated DataFrame\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "H5AQYNEMrBGY"
      },
      "outputs": [],
      "source": [
        "def clean_str(text):\n",
        "    search = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ى\",'\\n', '\\t','\"','?','؟','!']\n",
        "    replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\"\",\"\",\"\",\"ي\",\"\",' ', ' ',' ',' ? ',' ؟ ',' ! ']\n",
        "    # remove HTML TAG\n",
        "    html = re.compile('[<،,()\"\".#*?>]')\n",
        "    text = html.sub(r'',text)\n",
        "\n",
        "    # Remove urls:\n",
        "    url = re.compile('https?://\\S+|www\\.S+')\n",
        "    text = url.sub(r'',text)\n",
        "\n",
        "    # Remove email id:\n",
        "    email = re.compile('[A-Za-z0-2]+@[\\w]+.[\\w]+')\n",
        "    text = email.sub(r'',text)\n",
        "\n",
        "    # Remove tashkeel\n",
        "    p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
        "    text = re.sub(p_tashkeel,\"\", text)\n",
        "\n",
        "    # Remove longation \"EX : سلاممممم = سلامم\"\n",
        "    p_longation = re.compile(r'(.)\\1+')\n",
        "    subst = r\"\\1\\1\"\n",
        "    text = re.sub(p_longation, subst, text)\n",
        "\n",
        "    text = text.replace('وو', 'و')\n",
        "    text = text.replace('يي', 'ي')\n",
        "    text = text.replace('اا', 'ا')\n",
        "    for i in range(0, len(search)):\n",
        "        text = text.replace(search[i], replace[i])\n",
        "\n",
        "    # remove any leading and trailing whitespace characters\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "df['cleaned_text'] = df['content'].apply(clean_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "M7xvW-SvrEJd"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    stop_words = set(stopwords.words('arabic'))\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "def lemmatize(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(word) for word in tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmzhlTKuthxg",
        "outputId": "101f94bf-e14e-4929-9743-2f02f06398a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZCtsjT_drGVK"
      },
      "outputs": [],
      "source": [
        "df['tokens'] = df['cleaned_text'].apply(tokenize)\n",
        "df['tokens'] = df['tokens'].apply(remove_stopwords)\n",
        "df['tokens'] = df['tokens'].apply(lemmatize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVn6q-rktk_h",
        "outputId": "79f69fe1-cd90-4cce-bf47-775237c350f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0      [المحتوي, سنقدم, نظره, عامه, علي, الثقافه, الب...\n",
              "1      [المحتوي, سنقدم, نظره, عامه, علي, الثقافه, الب...\n",
              "2      [يستكشف, المحتوي, دور, الذكاء, الاصطناعي, مستق...\n",
              "3      [يتناول, المحتوي, تحليلا, لتطور, صناعه, الالعا...\n",
              "4      [يقدم, المحتوي, استعراضا, لتاثير, التغيرات, ال...\n",
              "                             ...                        \n",
              "223    [حاول, الغرب, علي, مدي, السنوات, القليله, الما...\n",
              "224    [يحذر, خبراء, جنوب, افريقيا, عوده, الرئيس, الس...\n",
              "225    [سياق, اقليمي, معقد, تمر, منطقه, افريقيا, جنوب...\n",
              "226    [قرر, يوفنتوس, الايطالي, الطعن, حكم, كريستيانو...\n",
              "227    [التحليل, سنستكشف, احدث, التطورات, مجال, تكنول...\n",
              "Name: tokens, Length: 228, dtype: object"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['tokens']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbM_mnXxl5Vj"
      },
      "source": [
        "### 3 ) Trainning our models using RNN, Bidirectional RNN, GRU and LSTM Architectures and"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "8sNumVqXu79-"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['tokens'])\n",
        "X = tokenizer.texts_to_sequences(df['tokens'])\n",
        "X = pad_sequences(X)\n",
        "y=df['rating']\n",
        "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.3, random_state=5760)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=9541)\n",
        "\n",
        "#5760 9541\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "RL8NLnThK9f-"
      },
      "outputs": [],
      "source": [
        "# Define model architectures\n",
        "def build_rnn_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=100, input_shape=input_shape),\n",
        "        LSTM(64),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_bidirectional_rnn_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=100, input_shape=input_shape),\n",
        "        Bidirectional(LSTM(64)),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_gru_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=100, input_shape=input_shape),\n",
        "        GRU(64),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_lstm_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=100, input_shape=input_shape),\n",
        "        LSTM(64),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Train the models\n",
        "def train_model(model, X_train, y_train, X_val, y_val, epochs=10, batch_size=64):\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdQ8AyZBtmDo",
        "outputId": "5ab6f541-108a-4086-ee4d-1eac5e00486a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3/3 [==============================] - 9s 2s/step - loss: 54.2612 - val_loss: 53.7523\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - 5s 2s/step - loss: 53.3151 - val_loss: 52.7528\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - 6s 1s/step - loss: 51.8224 - val_loss: 50.8300\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - 5s 1s/step - loss: 48.7803 - val_loss: 45.5660\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - 7s 2s/step - loss: 38.8260 - val_loss: 24.5579\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - 5s 1s/step - loss: 19.9025 - val_loss: 13.3155\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - 5s 1s/step - loss: 11.5656 - val_loss: 8.9471\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - 7s 2s/step - loss: 8.3146 - val_loss: 7.1114\n",
            "Epoch 9/10\n",
            "3/3 [==============================] - 5s 1s/step - loss: 7.0515 - val_loss: 6.4083\n",
            "Epoch 10/10\n",
            "3/3 [==============================] - 6s 2s/step - loss: 6.5486 - val_loss: 6.2803\n",
            "Epoch 1/10\n",
            "3/3 [==============================] - 15s 3s/step - loss: 54.0678 - val_loss: 52.5834\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - 11s 4s/step - loss: 51.8525 - val_loss: 49.8593\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - 9s 3s/step - loss: 48.5261 - val_loss: 45.0266\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - 10s 3s/step - loss: 42.0960 - val_loss: 34.0910\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - 11s 4s/step - loss: 27.4485 - val_loss: 14.0604\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - 9s 3s/step - loss: 11.2143 - val_loss: 6.8078\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - 9s 3s/step - loss: 6.6743 - val_loss: 6.5270\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - 11s 3s/step - loss: 6.9590 - val_loss: 7.2192\n",
            "Epoch 9/10\n",
            "3/3 [==============================] - 11s 4s/step - loss: 7.5489 - val_loss: 7.2141\n",
            "Epoch 10/10\n",
            "3/3 [==============================] - 8s 3s/step - loss: 7.3279 - val_loss: 6.7437\n",
            "Epoch 1/10\n",
            "3/3 [==============================] - 6s 1s/step - loss: 54.1613 - val_loss: 53.4979\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - 5s 2s/step - loss: 53.0976 - val_loss: 52.4715\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - 3s 1s/step - loss: 51.7408 - val_loss: 51.1527\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - 3s 1s/step - loss: 49.9568 - val_loss: 49.3493\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - 4s 1s/step - loss: 47.3836 - val_loss: 46.7331\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - 4s 1s/step - loss: 43.6474 - val_loss: 42.6240\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - 3s 1s/step - loss: 37.5791 - val_loss: 35.2774\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - 3s 1s/step - loss: 26.7165 - val_loss: 18.9889\n",
            "Epoch 9/10\n",
            "3/3 [==============================] - 5s 2s/step - loss: 9.4807 - val_loss: 5.9062\n",
            "Epoch 10/10\n",
            "3/3 [==============================] - 3s 1s/step - loss: 6.4823 - val_loss: 7.5101\n",
            "Epoch 1/10\n",
            "3/3 [==============================] - 8s 2s/step - loss: 54.2514 - val_loss: 53.8034\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - 5s 1s/step - loss: 53.3785 - val_loss: 52.8975\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - 4s 1s/step - loss: 52.0469 - val_loss: 51.3419\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - 7s 2s/step - loss: 49.6063 - val_loss: 48.0011\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - 4s 1s/step - loss: 43.9370 - val_loss: 35.7841\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - 4s 1s/step - loss: 26.3269 - val_loss: 14.1774\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - 6s 1s/step - loss: 11.1268 - val_loss: 7.2159\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - 4s 1s/step - loss: 6.7194 - val_loss: 6.2869\n",
            "Epoch 9/10\n",
            "3/3 [==============================] - 6s 2s/step - loss: 6.6902 - val_loss: 6.7238\n",
            "Epoch 10/10\n",
            "3/3 [==============================] - 5s 1s/step - loss: 7.0269 - val_loss: 6.9101\n",
            "2/2 [==============================] - 1s 180ms/step\n",
            "RNN Model MSE: 3.832188652322704\n",
            "2/2 [==============================] - 2s 280ms/step\n",
            "Bidirectional RNN Model MSE: 3.3764383610090163\n",
            "2/2 [==============================] - 1s 98ms/step\n",
            "GRU Model MSE: 3.3885796714971406\n",
            "2/2 [==============================] - 1s 112ms/step\n",
            "LSTM Model MSE: 3.33706244085546\n"
          ]
        }
      ],
      "source": [
        "# Build and train RNN model\n",
        "rnn_model = build_rnn_model(X_train.shape[1:])\n",
        "rnn_history = train_model(rnn_model, X_train, y_train, X_val, y_val)\n",
        "\n",
        "# Build and train Bidirectional RNN model\n",
        "bidirectional_rnn_model = build_bidirectional_rnn_model(X_train.shape[1:])\n",
        "bidirectional_rnn_history = train_model(bidirectional_rnn_model, X_train, y_train, X_val, y_val)\n",
        "\n",
        "# Build and train GRU model\n",
        "gru_model = build_gru_model(X_train.shape[1:])\n",
        "gru_history = train_model(gru_model, X_train, y_train, X_val, y_val)\n",
        "\n",
        "# Build and train LSTM model\n",
        "lstm_model = build_lstm_model(X_train.shape[1:])\n",
        "lstm_history = train_model(lstm_model, X_train, y_train, X_val, y_val)\n",
        "\n",
        "# Evaluate the models\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    return mse\n",
        "\n",
        "rnn_mse = evaluate_model(rnn_model, X_test, y_test)\n",
        "print(\"RNN Model MSE:\", rnn_mse)\n",
        "\n",
        "bidirectional_rnn_mse = evaluate_model(bidirectional_rnn_model, X_test, y_test)\n",
        "print(\"Bidirectional RNN Model MSE:\", bidirectional_rnn_mse)\n",
        "\n",
        "gru_mse = evaluate_model(gru_model, X_test, y_test)\n",
        "print(\"GRU Model MSE:\", gru_mse)\n",
        "\n",
        "lstm_mse = evaluate_model(lstm_model, X_test, y_test)\n",
        "print(\"LSTM Model MSE:\", lstm_mse)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
