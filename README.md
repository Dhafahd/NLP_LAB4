### MST AIDS 2023-2024 (DÃ©partement GÃ©nie Informatique)
**Subject : The main purpose behind this lab is to get familiar with ule-based techniques, Regex, and NLP Word Embedding.**\
**Realize by : Chibani Fahd**\
**web source : Aljarida24r**\
**Course : NLP**\

## Part1 : Classification Regression:
## Introduction
In this lab, we will embark on a comprehensive journey through the field of Natural Language Processing (NLP) using advanced machine learning techniques. Our primary objective is to develop a robust NLP pipeline and evaluate various language models. The process begins with collecting text data from several Arabic websites on a specific topic using web scraping libraries such as Scrapy and BeautifulSoup. This data will be organized into a dataset where each text is assigned a relevance score between 0 to 10, indicating its pertinence to the topic. Following data collection, we will preprocess the dataset through a series of NLP techniques, including tokenization, stemming, lemmatization, stop words removal, and discretization, to prepare it for model training. We will then train four different architecturesâ€”RNN, Bidirectional RNN, GRU, and LSTMâ€”while tuning hyper-parameters to optimize their performance. Finally, we will evaluate the models using standard metrics and additional measures like the BLEU score to determine their efficacy in handling the Arabic language dataset. This lab aims to provide a deep understanding of the capabilities and limitations of various NLP models in processing and analyzing text data in Arabic.
## 1. Scraping data :
### 1.1 Scraping using Selenuim :
We used the Selenium library to scrape links from the Al Jazeera website related to the Palestinian war. By automating our web scraping process with Selenium, we efficiently navigated through various web pages, dynamically loaded content, and extracted the necessary link
```python
Exemple of links scraped : ['https://www.aljazeera.net/news/2024/5/25/%D8%A3%D9%88%D9%84%D9%85%D8%B1%D8%AA-2']
```
Using this library we scrapted more than 304 links related to the Palestinian war
### 1.2 Scraping using BeautifulSoup :
We used the Selenium library to scrape links from the Al Jazeera website related to the Palestinian war. After gathering the links, we employed the BeautifulSoup library to extract the content of each article. BeautifulSoup allowed us to parse the HTML structure of the web pages, efficiently retrieving text, images, and other relevant data. Once the content was extracted, we organized it and stored it in a JSON file, ensuring a structured and easily accessible format for further analysis. This combination of Selenium for link extraction and BeautifulSoup for content retrieval enabled us to create a comprehensive and well-organized dataset of articles on the Palestinian war.
```python
Content : Ø£ÙƒØ¯Øª Ù…Ù†Ø¸Ù…Ø© Ø§Ù„ØµØ­Ø© Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ø§Ù„ÙŠÙˆÙ… Ø§Ù„Ø«Ù„Ø§Ø«Ø§Ø¡ Ø«Ù‚ØªÙ‡Ø§ Ø§Ù„ÙƒØ§Ù…Ù„Ø© ÙÙŠ Ø¥Ø­ØµØ§Ø¡Ø§Øª ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØµØ­Ø© ÙÙŠ Ù‚Ø·Ø§Ø¹ ØºØ²Ø© Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø´Ù‡Ø¯Ø§Ø¡ Ø§Ù„Ø°ÙŠÙ† Ø³Ù‚Ø·ÙˆØ§ ÙÙŠ Ø§Ù„Ø¹Ø¯ÙˆØ§Ù† Ø§Ù„Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø¨Ø¹Ø¯Ù…Ø§ Ø´ÙƒÙƒØª Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ ÙÙŠ Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¯.  ÙˆØ­Ø¯Ø«Øª ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØµØ­Ø© ÙÙŠ Ø§Ù„Ù‚Ø·Ø§Ø¹ Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ Ø§Ù„Ù…Ø§Ø¶ÙŠ ØªØ­Ù„ÙŠÙ„Ù‡Ø§ Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø´Ù‡Ø¯Ø§Ø¡ Ø§Ù„Ø¨Ø§Ù„Øº Ù†Ø­Ùˆ 35 Ø£Ù„ÙØ§ØŒ ÙˆÙ‚Ø§Ù„Øª Ø¥Ù†Ù‡ ØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¨Ø´ÙƒÙ„ ÙƒØ§Ù…Ù„ Ø¹Ù„Ù‰ Ù‡ÙˆÙŠØ§Øª Ø­ÙˆØ§Ù„ÙŠ 25 Ø£Ù„ÙØ§ Ù…Ù†Ù‡Ù… Ø­ØªÙ‰ Ø§Ù„Ø¢Ù†. ÙˆØ£Ø¹Ø§Ø¯Øª ÙˆÙƒØ§Ù„Ø§Øª Ø§Ù„Ø£Ù…Ù…Â Ø§Ù„Ù…ØªØ­Ø¯Ø© Ù†Ø´Ø± Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø±Ù‚Ø§Ù….  ÙˆÙ‚Ø§Ù„ Ø§Ù„Ù…ØªØ­Ø¯Ø« Ø¨Ø§Ø³Ù… Ù…Ù†Ø¸Ù…Ø© Ø§Ù„ØµØ­Ø© Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© ÙƒØ±ÙŠØ³ØªÙŠØ§Ù† Ù„ÙŠÙ†Ø¯Ù…Ø§ÙŠØ± ÙÙŠ Ù…Ø¤ØªÙ…Ø± ØµØ­ÙÙŠ ÙÙŠ Ø¬Ù†ÙŠÙ Ø¥Ù† \"Ø­Ù‚ÙŠÙ‚Ø© Ø£Ù† Ù„Ø¯ÙŠÙ†Ø§ Ø§Ù„Ø¢Ù† 25 Ø£Ù„Ù Ø´Ø®Øµ ØªÙ… Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ù‡ÙˆÙŠØªÙ‡Ù… Ù‡ÙŠ Ø®Ø·ÙˆØ© Ø¥Ù„Ù‰ Ø§Ù„Ø£Ù…Ø§Ù…\"ØŒ ÙˆØªØ§Ø¨Ø¹ Ø£Ù†Ù‡ \"Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø®Ø·Ø£\" ÙÙŠ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØµØ­Ø©. 

```
## 2. Score data :
Once the content was extracted, we organized it and stored it in a JSON file, ensuring a structured and easily accessible format for further analysis. To evaluate the relevance of each article to our topic, we implemented a scoring system using a classification pipeline with a pre-trained model for zero-shot classification specifically designed for Arabic text. This model provided a rating based on the similarity of each article to the Palestinian war topic, allowing us to prioritize the most pertinent articles for our research.
```python
Content: ['Ù‚Ø±Ø± ÙŠÙˆÙÙ†ØªÙˆØ³ Ø§Ù„Ø¥ÙŠØ·Ø§Ù„ÙŠ Ø§Ù„Ø·Ø¹Ù† ÙÙŠ Ø­ÙƒÙ… ÙƒØ±ÙŠØ³ØªÙŠØ§Ù†Ùˆ Ø±ÙˆÙ†Ø§Ù„Ø¯Ùˆ Ù„Ø§Ø¹Ø¨ Ø§Ù„Ù†ØµØ± Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ Ø¨Ø¹Ø¯ ÙÙˆØ²Ù‡ Ø±Ø³Ù…ÙŠØ§ ÙÙŠ Ø§Ù„Ù…Ø¹Ø±ÙƒØ© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…Ø¹ Ø§Ù„Ù†Ø§Ø¯ÙŠ Ø¨Ø´Ø£Ù† Ø§Ù„Ø±Ø§ØªØ¨.  ÙˆØ£ÙƒØ¯ Ø§Ù„ØµØ­ÙÙŠ Ø§Ù„Ø¥ÙŠØ·Ø§Ù„ÙŠ Ø§Ù„Ø´Ù‡ÙŠØ± ÙØ§Ø¨Ø±ÙŠØ²ÙŠÙˆ Ø±ÙˆÙ…Ø§Ù†Ùˆ Ø¹Ø¨Ø± Ø­Ø³Ø§Ø¨Ù‡ Ø¹Ù„Ù‰ Ù…Ù†ØµØ© \"Ø¥ÙƒØ³\" Ø£Ù† ÙŠÙˆÙÙ†ØªÙˆØ³ Ø³ÙŠØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù„ØªØ¬Ù†Ø¨ Ø¯ÙØ¹ Ù…Ø¨Ù„Øº 9.8 Ù…Ù„Ø§ÙŠÙŠÙ† ÙŠÙˆØ±Ùˆ Ø±ÙˆØ§ØªØ¨ Ù…ØªØ£Ø®Ø±Ø© Ù„Ø±ÙˆÙ†Ø§Ù„Ø¯Ùˆ Ø¹Ù† Ù…ÙˆØ³Ù… 2020-2021ØŒ Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø§Ù„ÙÙˆØ§Ø¦Ø¯.    ÙˆÙŠØµØ± Ù†Ø§Ø¯ÙŠ \"Ø§Ù„Ø³ÙŠØ¯Ø© Ø§Ù„Ø¹Ø¬ÙˆØ²\" Ø¹Ù„Ù‰ Ù…ÙˆÙ‚ÙÙ‡ Ø¨Ø£Ù† Ø±ÙˆÙ†Ø§Ù„Ø¯Ùˆ Ù„ÙŠØ³ Ù„Ù‡ Ø§Ù„Ø­Ù‚ ÙÙŠ Ø§Ù„Ø£Ù…ÙˆØ§Ù„ Ø§Ù„Ù…ØªÙ†Ø§Ø²Ø¹ Ø¹Ù„ÙŠÙ‡Ø§ØŒ Ù„Ø£Ù†Ù‡ Ù„Ù… ÙŠØªÙ… ØªÙˆÙ‚ÙŠØ¹ Ø£ÙŠ Ø¹Ù‚ÙˆØ¯ Ø¬Ø¯ÙŠØ¯Ø© ØªØªØ¹Ù„Ù‚ Ø¨Ø§Ù„ØªÙ†Ø§Ø²Ù„Ø§Øª Ø¹Ù† Ø§Ù„Ø±ÙˆØ§ØªØ¨ Ø®Ù„Ø§Ù„ Ø¬Ø§Ø¦Ø­Ø© ÙƒÙˆØ±ÙˆÙ†Ø§ØŒ Ø¨ÙŠÙ†Ù…Ø§ Ø­ØµÙ„ Ø§Ù„Ù„Ø§Ø¹Ø¨ÙˆÙ† Ø§Ù„Ø¢Ø®Ø±ÙˆÙ† Ø§Ù„Ø°ÙŠÙ† ÙˆÙ‚Ø¹ÙˆØ§ Ø¹Ù„Ù‰ ØªÙ„Ùƒ Ø§Ù„Ø§ØªÙØ§Ù‚ÙŠØ§Øª Ø¹Ù„Ù‰ Ù…Ø³ØªØ­Ù‚Ø§ØªÙ‡Ù….  ÙˆÙˆØ§ÙÙ‚ Ù„Ø§Ø¹Ø¨Ùˆ ÙŠÙˆÙÙ†ØªÙˆØ³ Ø¹Ù„Ù‰ ØªØ£Ø¬ÙŠÙ„ Ø±ÙˆØ§ØªØ¨Ù‡Ù… Ù„Ù…Ø¯Ø© 4 Ø£Ø´Ù‡Ø± Ø®Ù„Ø§Ù„ Ø§Ù„ÙØªØ±Ø© Ù…Ù† Ù…Ø§Ø±Ø³/Ø¢Ø°Ø§Ø± 2020 ÙˆØ­ØªÙ‰ Ø£Ø¨Ø±ÙŠÙ„/Ù†ÙŠØ³Ø§Ù† 2021ØŒ Ø­ÙŠØ« ÙƒØ§Ù† Ø§Ù„Ù†Ø§Ø¯ÙŠ ÙŠØ¹Ø§Ù†ÙŠ Ù…Ø§Ù„ÙŠØ§ØŒ ÙˆÙ„ÙƒÙ† ØªÙ… Ø£ÙŠØ¶Ø§ Ø¥Ø¨Ø±Ø§Ù… Ø§ØªÙØ§Ù‚ÙŠØ§Øª ÙØ±Ø¯ÙŠØ© Ù…Ø¹ Ø§Ù„Ù„Ø§Ø¹Ø¨ÙŠÙ†.  ÙˆØ²Ø¹Ù… ÙŠÙˆÙÙ†ØªÙˆØ³ Ø£ÙŠØ¶Ø§ Ø£Ù† Ø±ÙˆÙ†Ø§Ù„Ø¯Ùˆ ØªÙ†Ø§Ø²Ù„ Ø¹Ù† Ø£Ù…ÙˆØ§Ù„Ù‡ Ø§Ù„Ù…Ø³ØªØ­Ù‚Ø© Ø¹Ù†Ø¯Ù…Ø§ ØºØ§Ø¯Ø± Ø¥Ù„Ù‰ Ù…Ø§Ù†Ø´Ø³ØªØ± ÙŠÙˆÙ†Ø§ÙŠØªØ¯ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ ÙÙŠ ØµÙŠÙ Ø¹Ø§Ù… 2021.  ÙˆØ·Ø§Ù„Ø¨ Ø±ÙˆÙ†Ø§Ù„Ø¯ÙˆØŒ Ø§Ù„Ø°ÙŠ Ù„Ø¹Ø¨ ÙÙŠ ØµÙÙˆÙ ÙŠÙˆÙÙ†ØªÙˆØ³ 3 Ù…ÙˆØ§Ø³Ù… (2018-2021) Ù‚Ø¨Ù„ Ø§Ù„Ø¹ÙˆØ¯Ø© Ø¥Ù„Ù‰ Ù…Ø§Ù† ÙŠÙˆÙ†Ø§ÙŠØªØ¯ (2021-2022) ÙˆÙ…Ù†Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†ØµØ± Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØŒ Ø¨Ù€19.5 Ù…Ù„ÙŠÙˆÙ† ÙŠÙˆØ±ÙˆØŒ Ù„ÙƒÙ† Ø§Ù„Ù…Ø­ÙƒÙ…Ø© Ø®ÙÙ‘Ø¶Øª Ø§Ù„Ù…Ø¨Ù„Øº Ø¨Ù†Ø³Ø¨Ø© 50%.  ÙˆÙˆÙÙ‚Ø§ Ù„Ù…Ø¬Ù„Ø© Ø§Ù„Ø£Ø¹Ù…Ø§Ù„ Ø§Ù„Ø£Ù…ÙŠØ±ÙƒÙŠØ© ÙÙˆØ±Ø¨Ø³ØŒ ÙƒØ§Ù† Ø±ÙˆÙ†Ø§Ù„Ø¯ÙˆØŒ Ø§Ù„ÙØ§Ø¦Ø² Ø¨Ø¬Ø§Ø¦Ø²Ø© Ø§Ù„ÙƒØ±Ø© Ø§Ù„Ø°Ù‡Ø¨ÙŠØ© 5 Ù…Ø±Ø§ØªØŒ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠ Ø§Ù„Ø£Ø¹Ù„Ù‰ Ø£Ø¬Ø±Ø§ ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù… Ø®Ù„Ø§Ù„ Ø¹Ø§Ù… 2023ØŒ Ø¨Ù…Ø¨Ù„Øº 136 Ù…Ù„ÙŠÙˆÙ† Ø¯ÙˆÙ„Ø§Ø±ØŒ Ù…Ù† Ø¨ÙŠÙ†Ù‡Ø§ 46 Ù…Ù„ÙŠÙˆÙ† Ø¯ÙˆÙ„Ø§Ø± Ø±ÙˆØ§ØªØ¨.    ÙˆØ³Ø¨Ù‚ Ø£Ù† Ø£Ø¹Ù„Ù† ÙŠÙˆÙÙ†ØªÙˆØ³ØŒ Ø§Ù„Ø£ÙƒØ«Ø± ØªØªÙˆÙŠØ¬Ø§ ÙÙŠ Ø¥ÙŠØ·Ø§Ù„ÙŠØ§ØŒ ÙÙŠ Ø£ÙƒØªÙˆØ¨Ø±/ØªØ´Ø±ÙŠÙ† Ø§Ù„Ø£ÙˆÙ„ Ø§Ù„Ù…Ø§Ø¶ÙŠØŒ Ø¹Ù† Ø®Ø³Ø§Ø¦Ø± Ø¨Ù„ØºØª 123.7 Ù…Ù„ÙŠÙˆÙ† ÙŠÙˆØ±Ùˆ ÙÙŠ Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ù…Ø§Ù„ÙŠØ© 2022-2023ØŒ Ø§Ù„ØªÙŠ Ø§Ù…ØªØ¯Øª Ø­ØªÙ‰ Ù†Ù‡Ø§ÙŠØ© ÙŠÙˆÙ†ÙŠÙˆ/Ø­Ø²ÙŠØ±Ø§Ù†.  ÙˆØ£Ù…Ø¶Ù‰ Ø±ÙˆÙ†Ø§Ù„Ø¯Ùˆ 3 Ø³Ù†ÙˆØ§Øª ÙÙŠ ØªÙˆØ±ÙŠÙ†Ùˆ Ø¨Ø¹Ø¯ Ù…ØºØ§Ø¯Ø±ØªÙ‡ Ø±ÙŠØ§Ù„ Ù…Ø¯Ø±ÙŠØ¯ ÙÙŠ 2018ØŒ ÙˆØ³Ø¬Ù„ Ø®Ù„Ø§Ù„Ù‡Ø§ 101 Ù‡Ø¯Ù ÙÙŠ 134 Ù…Ø¨Ø§Ø±Ø§Ø© Ù…Ø¹ ÙŠÙˆÙÙ†ØªÙˆØ³ØŒ ÙˆØ³Ø§Ù‡Ù… Ù…Ø¹ \"Ø§Ù„Ø³ÙŠØ¯Ø© Ø§Ù„Ø¹Ø¬ÙˆØ²\" ÙÙŠ Ø§Ù„ÙÙˆØ² Ø¨Ù„Ù‚Ø¨ÙŠÙ† ÙÙŠ Ø§Ù„Ø¯ÙˆØ±ÙŠ Ø§Ù„Ø¥ÙŠØ·Ø§Ù„ÙŠ ÙˆÙƒØ£Ø³ Ø¥ÙŠØ·Ø§Ù„ÙŠØ§.']
Rating :0.3990756273
```

### 3.1 Data pre-procecing and modeling :
We established a comprehensive preprocessing NLP pipeline for our collected dataset, incorporating essential steps such as tokenization, stemming, lemmatization, stop words removal, and discretization. These preprocessing steps ensured that the textual data was clean, normalized, and ready for effective analysis. Following the preprocessing phase, we trained our models using various advanced architectures, including Recurrent Neural Networks (RNN), Bidirectional RNNs, Gated Recurrent Units (GRU), and Long Short-Term Memory (LSTM) networks. Each model's hyperparameters were meticulously tuned to achieve the best possible performance. To evaluate the effectiveness of our models, we employed standard metrics such as accuracy, precision, recall, and F1-score. Additionally, we used other relevant metrics like the BLEU score to assess the quality and relevance of the language models, providing a comprehensive evaluation of their performance across different dimensions. This thorough approach ensured that our models were robust, accurate, and capable of handling the intricacies of the dataset.
## Part 2 : Transformer (Text generation):


In this section, we explore text generation using Transformer models, specifically focusing on the powerful capabilities of GPT-2. We'll start by installing the pytorch-transformers library and loading the GPT-2 pre-trained model. The pre-trained GPT-2 model serves as a robust foundation due to its extensive training on diverse and large-scale datasets, making it proficient in understanding and generating human-like text.

Next, we'll fine-tune the GPT-2 model to a customized dataset (Recipes dataset). This step involves adapting the pre-trained model to a specific context or domain, enhancing its performance and relevance for our particular use case. Fine-tuning allows us to leverage the general knowledge encoded in GPT-2 while tailoring it to generate content that aligns with our customized dataset.

Finally, we'll utilize the fine-tuned model to generate new recipes based. This task demonstrates the practical application of GPT-2 in text generation, showcasing its ability to produce coherent and contextually appropriate content. By the end of this section, you'll gain hands-on experience in fine-tuning a Transformer model and generating high-quality text tailored to specific prompts.

```python
Example of recipes generated by the trained model :

Recipe 1 :1/2 cup All Purpose Flour (Maida),1/2 teaspoon Active dry yeast,1/2 teaspoon Salt,2 tablespoons Sugar,1/2 cup Extra Virgin Olive Oil,1/2 cup Lukewarm Water,1 tablespoon Butter,3 cloves Garlic - chopped,1 teaspoon Dried oregano,Salt - to taste,1/4 cup Parmesan cheese - grated 
Recipe 2 :1 Cauliflower (gobi) - cut into florets,1 cup Green beans (French Beans) - cut into 1 inch pieces,1 Onion - finely chopped,2 cloves Garlic - finely chopped,1 inch Ginger - finely chopped,1/2 teaspoon Turmeric powder (Haldi),1 teaspoon Red Chilli powder,Salt - to taste,1 tablespoon Ghee,1 teaspoon Cumin seeds (Jeera)
Recipe 3 :1 cup All Purpose Flour (Maida),3/4 cup Butter (unsalted),1 teaspoon Baking powder,1 teaspoon Baking soda,3/4 teaspoon Salt,3 Whole Eggs,2 tablespoon Milk,1/2 cup Curd (Dahi / Yogurt)

Feel free to try them ğŸ˜ğŸ˜
```

## Part 3 : BERT 
In this segment, we delve into analyzing Amazon Fashion ratings using the powerful BERT (Bidirectional Encoder Representations from Transformers) model. Our objective is to leverage the pre-trained bert-base-uncased model for sentiment analysis, with the reviewText as features and overall ratings as targets.

To kickstart our analysis, we'll initialize the BERT model and prepare our data, ensuring compatibility with the BERT embedding layer. This step involves tokenizing the text data and adapting it to fit the input requirements of the BERT model.

Next, we'll fine-tune and train our BERT model, carefully selecting hyperparameters to optimize its performance. Fine-tuning enables us to adapt the pre-trained BERT model to our specific task, enhancing its ability to understand and classify sentiment in Amazon Fashion reviews effectively.

Once trained, we'll evaluate the model's performance using standard metrics such as accuracy, loss, and F1 score. Additionally, we'll explore other relevant metrics like the BLEU score and BERT-specific metrics to gain deeper insights into the model's capabilities.

Finally, we'll draw a general conclusion regarding the effectiveness and utility of using a pre-trained BERT model for sentiment analysis tasks, highlighting its strengths and potential areas for improvement. This analysis provides valuable insights into the practical applications of BERT in sentiment analysis and its overall impact on understanding Amazon Fashion ratings.
```python
Testing our model  :

Input : it was normal
Predicted rating: 4.01

```

## Conclusion
Through the various parts of our project, we have gained comprehensive insights into the powerful capabilities of modern natural language processing (NLP) models and techniques. Initially, by scraping and processing data using tools like Selenium and BeautifulSoup, we understood the importance of effective data collection and preprocessing in building reliable NLP applications. Establishing a preprocessing pipeline enabled us to clean and normalize text data, which is crucial for accurate model training.

In the text generation segment, we learned how to leverage the pre-trained GPT-2 model, fine-tuning it to generate coherent and contextually relevant paragraphs. This demonstrated the practical application of transformer models in generating high-quality text content, highlighting their versatility and robustness.

Transitioning to sentiment analysis, we explored the use of the pre-trained BERT model to analyze Amazon Fashion ratings. We fine-tuned BERT to our specific dataset, which illustrated the adaptability of pre-trained models to new tasks and domains. By evaluating the model with standard and advanced metrics, we could measure its performance comprehensively, understanding both its strengths and limitations.

Overall, our journey through these various parts emphasized the significance of combining data preprocessing, model fine-tuning, and thorough evaluation to achieve efficient and effective NLP solutions. We saw firsthand the impact of using pre-trained models like GPT-2 and BERT, which offer state-of-the-art performance and flexibility, making them invaluable tools for a wide range of NLP applications.

<a id="1">[1]</a>Jeet. (2021, December 15). One Hot encoding of text data in Natural Language Processing. Medium. https://medium.com/analytics-vidhya/one-hot-encoding-of-text-data-in-natural-language-processing-2242fefb2148 \
<a id="2">[2]</a> Alkhatib, R. M., Zerrouki, T., Shquier, M. M. A., & Balla, A. (2023). Tashaphyne0.4: A new Arabic light stemmer based on rhizome modeling approach. Information Retrieval Journal, 26(14). doi: https://doi.org/10.1007/s10791-023-09429-y \
<a id="3">[3]</a>  notebook.community. (n.d.). https://notebook.community/arcyfelix/Courses/18-03-07-Deep%20Learning%20With%20Python%20by%20Fran%C3%A7ois%20Chollet/.ipynb_checkpoints/Chapter%206.1.1%20-%20One-hot%20encoding%20of%20words%20and%20characters-checkpoint/ \
<a id="4">[4]</a> MagedSaeed. (n.d.). GitHub - MagedSaeed/farasapy: A Python implementation of Farasa toolkit. GitHub. https://github.com/MagedSaeed/farasapy?tab=readme-ov-file#want-to-cite \
<a id="5">[5]</a> Munther, I. (2021, December 30). Sentiment Analysis of Arabic Text Data (Tweets) - Analytics Vidhya - Medium. Medium. https://medium.com/analytics-vidhya/sentiment-analysis-of-arabic-text-data-tweets-4e96c8da892b \
<a id="6">[6]</a> freeCodeCamp.org. (2019, July 24). How to process textual data using TF-IDF in Python. https://www.freecodecamp.org/news/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3/ \
<a id="7">[7]</a>  GeeksforGeeks. (2024, January 3). Word Embedding using Word2Vec. GeeksforGeeks. https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/

